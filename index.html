<!--  -->
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="seal_icon.png">
  <title>Henry (Yuhao) Zhou</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="67%" valign="middle">
          <p align="center">
            <name>Henry (Yuhao) Zhou</name>
          </p>
          <p>
            I have recently finished my undergraduate study at  
            <a href="https://www.utoronto.ca/">
            University of Toronto. 
            </a>
            and joined Facebook AI Research as an AI Resident working with Michael Auli and Alexei Baevski on unsupervised speech pretraining. <br><br>
            As an undergraduate student, my focus of study are machine learning, software engineering and system control.
            Since January in 2017, I have been working as an undergraduate research assistant under the supervision of 
            <a href="http://www.cs.utoronto.ca/~fidler/index.html"> Prof. Sanja Fidler </a> and 
            <a href="https://jimmylba.github.io/"> Prof. Jimmy Ba </a>
            on computer vison and reinforcement learning projects.
          </p>
          <p>
            Before I worked as a research assistant, I primarily spent my time on various software engineering internships.
            During which period, I gained strong coding skills through in-depth experience on large-scale engineering projects. 
          </p>
          
          </td>
          <td width="33%">
            <img src="images/profile_pic.jpg" width="200">
          </td>



        </tr>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="33%">
            <p align=center>
              Address: <br>
              RM1801, 24 Wellesley St. West <br>
              Toronto, Canada. M4Y 2X6
            </p>
          </td>
          <td width="33%">
            <p align=center>
              Email: <br>
              henryzhou-at-cs-dot-toronto-dot-edu <br>
              henry-dot-zhou-at-mail-dot-utoronto-dot-ca
            </p>
          </td>
          
        </tr>
        <tr>
          <p align=center>
          <a href="docs/cv.pdf">CV</a> &nbsp/&nbsp
          <a href="https://www.linkedin.com/in/yuhao-henry-zhou-3b0747a8/"> LinkedIn </a> &nbsp/&nbsp
          <a href="https://github.com/HenryZhou7">GitHub</a> &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AJsN-F4R4Qg-wL0Q0FEVJvWi1iSUfsiFNIleGJMpOu9yonslcsFONDzWjjCMNb0bhtFo6FEdzReJjEHCGZoplRA9d5f8In1uLA&user=9-6xvKYAAAAJ">Google Scholar</a>
        </p>
        </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <heading>Education</heading>
          </td>
        </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr bgcolor="#ffffff">
          <td width="25%">
            <img src="images/education/uoft.png" width='250'></div>   
          </td>
          <td valign="top" width="75%">
            <p>
              <a href="https://www.utoronto.ca/"><strong>University of Toronto</strong></a><br>
              <a href="https://www.ece.utoronto.ca/">Faculty of Applied Science and Engineering</a><br>
              Bachelor of Applied Science <br> 
              &nbsp &nbsp &nbsp Specialist in Electrical and Computer Engineering<br>
              &nbsp &nbsp &nbsp <em>with</em> Robotics and Mechatronics Minor <br><br>
              Graduated with High Honour in June, 2019.<br>
              <em> Summa Cum Laude</em> <br>

              <br>
              <em>Cumulative GPA</em>: 3.92 / 4.00
            </p>
            

          </td>
        </tr>
      </table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <heading>Research</heading>
            <p>
              I am broadly interested in machine learning, deep learning and its application in computer vision, natural language processing, speech, and locomotion control.
              (* Denotes equal contribution.)
            </p>
          </td>
        </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

        <tr bgcolor="#ffffff">
          <td width="25%">
            <img src="images/research/gamegan-motivation.gif" width='250'></div>   
          </td>
          <td valign="top" width="75%">
            <p>
              <papertitle>Learning to Simulate Dynamic Environments with GameGAN</papertitle>
              <br>
              Seung Wook Kim, <strong>Yuhao Zhou</strong>, Jonah Philion, Antonio Torralba, Sanja Fidler<br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2020 <br>
              <a id='gamegan_abstract', onclick="detail_switch(this.id)">Abstract</a> /
              <a id='gamegan_bib' onclick='detail_switch(this.id)'>Bibtex</a> /
              <a href="https://nv-tlabs.github.io/gameGAN/">Project Web</a> /
              <a href="https://www.nvidia.com/en-us/research/ai-playground/">Live Demos</a> <br>
              Media:
              <a href="https://twitter.com/nvidia/status/1263818505742200833?s=20">Nvidia Twitter</a> /
              <a href="https://blogs.nvidia.com/blog/2020/05/22/gamegan-research-pacman-anniversary/">Blog Post</a> /
              <a href="https://www.youtube.com/watch?v=H8F6J7mYyz0&fbclid=IwAR1SxxWJBPgSzDCEWz4w11B51vSaJeE7zGELHlrSG6w_ej3l_GAGZdZLU2U">YouTube Explained </a> /
              <!-- <a href="https://openreview.net/forum?id=BkgWHnR5tm"> Open Review</a> / -->
              <!-- <a href='https://github.com/WilsonWangTHU/neural_graph_evolution'> Codes</a> /  -->
              <!-- <a href='http://www.cs.toronto.edu/~henryzhou/NGE_website/'>Project Web</a> / -->
              

              <br>
            </p>

            <p>
              <div id='gamegan_abstract_content' style='display:none;'>
                Simulation is a crucial component of any robotic system. In order to simulate correctly, we need to write complex rules of the environment: how dynamic agents behave, and how the actions of each of the agents affect the behavior of others. In this paper, we aim to learn a simulator by simply watching an agent interact with an environment. We focus on graphics games as a proxy of the real environment. We introduce GameGAN, a generative model that learns to visually imitate a desired game by ingesting screenplay and keyboard actions during training. Given a key pressed by the agent, GameGAN "renders" the next screen using a carefully designed generative adversarial network. Our approach offers key advantages over existing work: we design a memory module that builds an internal map of the environment, allowing for the agent to return to previously visited locations with high visual consistency. In addition, GameGAN is able to disentangle static and dynamic components within an image making the behavior of the model more interpretable, and relevant for downstream tasks that require explicit reasoning over dynamic elements. This enables many interesting applications such as swapping different components of the game to build new games that do not exist. We implement our approach as a web application enabling human players to now play Pacman and its generated variations with our GameGAN.
              </div>

              <div id='gamegan_bib_content' style="font-family:monospace;display:none;">
                @inproceedings{Kim2020_GameGan,</br>  
                author = {Seung Wook Kim and Yuhao Zhou and Jonah Philion and Antonio Torralba and Sanja Fidler},</br>  
                title = {{Learning to Simulate Dynamic Environments with GameGAN}},</br>  
                year = {2020},</br>  
                booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},</br>  
                month = {Jun.},</br>  doi = {}</br>}
              </div>
            </p>
            <p></p>

          </td>
        </tr>

        <tr bgcolor="#ffffff">
          <td width="25%">
            <img src="http://www.cs.toronto.edu/~tingwuwang/nge_title.png" width='250'></div>   
          </td>
          <td valign="top" width="75%">
            <p>
              <papertitle>Neural Graph Evolution: Automatic Robot Design</papertitle>
              <br>
              <strong>Yuhao Zhou</strong>*, Tingwu Wang*, Sanja Fidler, Jimmy Ba<br>
              <em>International Conference on Learning Representations</em>, 2019 <br>
              <a id='nge_abstract', onclick="detail_switch(this.id)">Abstract</a> /
              <a id='nge_bib' onclick='detail_switch(this.id)'>Bibtex</a> /
              <a href="https://openreview.net/forum?id=BkgWHnR5tm"> Open Review</a> /
              <a href='https://github.com/WilsonWangTHU/neural_graph_evolution'> Codes</a> / 
              <a href='http://www.cs.toronto.edu/~henryzhou/NGE_website/'>Project Web</a> /
              

              <br>
            </p>

            <p>
              <div id='nge_abstract_content' style='display:none;'>
                Despite the recent successes in robotic locomotion control, the design of robot relies heavily on human engineering. 
                Automatic robot design has been a long studied subject, but the recent progress has been slowed due to the large combinatorial search space and the difficulty in evaluating the found candidates. 
                To address the two challenges, we formulate automatic robot design as a graph search problem and perform evolution search in graph space. 
                We propose Neural Graph Evolution (NGE), which performs selection on current candidates and evolves new ones iteratively. 
                Different from previous approaches, NGE uses graph neural networks to parameterize the control policies, which reduces evaluation cost on new candidates with the help of skill transfer from previously evaluated designs. 
                In addition, NGE applies Graph Mutation with Uncertainty (GM-UC) by incorporating model uncertainty, which reduces the search space by balancing exploration and exploitation. 
                We show that NGE significantly outperforms previous methods by an order of magnitude. 
                As shown in experiments, NGE is the first algorithm that can automatically discover kinematically preferred robotic graph structures, such as a fish with two symmetrical flat side-fins and a tail, or a cheetah with athletic front and back legs. 
                Instead of using thousands of cores for weeks, NGE efficiently solves searching problem within a day on a single 64 CPU-core Amazon EC2.
              </div>

              <div id='nge_bib_content' style="font-family:monospace;display:none;">
                @inproceedings{
                wang2018neural,
                title={Neural Graph Evolution: Automatic Robot Design},
                author={Tingwu Wang and Yuhao Zhou and Sanja Fidler and Jimmy Ba},
                booktitle={International Conference on Learning Representations},
                year={2019},
                url={https://openreview.net/forum?id=BkgWHnR5tm},
                }
              </div>
            </p>
            <p></p>

          </td>
        </tr>

        <tr bgcolor="#ffffff">
          <td width="25%">
            <img src="images/research/movie4d.jpg" width='250'></div>   
          </td>
          <td valign="top" width="75%">
            <p>
              <a href="http://www.cs.toronto.edu/~henryzhou/movie4d/">
                <papertitle>Now You Shake Me: Towards Automatic 4D Cinema</papertitle>
              </a><br>
              <strong>Yuhao Zhou</strong>, Makarand Tapaswi, Sanja Fidler<br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2018 <strong>(Spotlight)</strong> <br>
              <a id='movie4d_abstract', onclick="detail_switch(this.id)">Abstract</a> /
              <a id='movie4d_bib' onclick='detail_switch(this.id)'>Bibtex</a> /
              <a href='http://www.cs.toronto.edu/~henryzhou/movie4d/'>Project Web</a> /
              <a href='http://www.cs.toronto.edu/~henryzhou/movie4d/sources/cvpr2018.pdf'>PDF</a> /
              <a href="https://www.youtube.com/watch?v=O92bGCTxul8&feature=youtu.be&t=5550">CVPR Spotlight</a> /
              <a href="http://www.cs.toronto.edu/~henryzhou/movie4d/sources/movie4d_poster.pdf">Poster</a> 
              <br>
              Media:
              <a href="https://www.utoronto.ca/news/experience-what-characters-are-feeling-u-t-researchers-use-ai-add-4d-effects-movies"> UofT News</a> / 
              <a href="http://www.cbc.ca/listen/shows/here-and-now-toronto/segment/15555905">CBC Radio News</a> /
              <a href="https://www.inquisitr.com/4976832/ai-algorithm-turns-regular-movies-into-4d-motion-pictures/">Inquisitr News</a>
            </p>

            <p>
              <div id='movie4d_abstract_content' style='display:none;'>
                We are interested in enabling automatic 4D cinema by parsing physical and special effects from untrimmed movies. 
                These include effects such as physical interactions, water splashing, light, and shaking, and are grounded to either a character in the scene or the camera. 
                We collect a new dataset referred to as the Movie4D dataset which annotates over 9K effects in 63 movies. 
                We propose a Conditional Random Field model atop a neural network that brings together visual and audio information, as well as semantics in the form of person tracks. Our model further exploits correlations of effects between different characters in the clip as well as across movie threads. 
                We propose effect detection and classification as two tasks, and present results along with ablation studies on our dataset, paving the way towards 4D cinema in everyone&#39;s homes.
              </div>

              <div id='movie4d_bib_content' style="font-family:monospace;display:none;">
                @inproceedings{Zhou2017_Movie4D,</br>  
                author = {Yuhao Zhou and Makarand Tapaswi and Sanja Fidler},</br>  
                title = {{Now You Shake Me: Towards Automatic 4D Cinema}},</br>  
                year = {2018},</br>  
                booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},</br>  
                month = {Jun.},</br>  doi = {}</br>}
              </div>
            </p>

            
            
            <p></p>

          </td>
        </tr>
      </table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td>
          <heading>Industry Experience</heading>
          <p>
            Please email me for more details of the experience. <br>
          </p>
          </td>
        </tr>
        
      </table>
      
      <table width="100%" align="center" border="0" cellpadding="20">
        
        <tr>
          <td width="25%"><img src="images/industry/facebookai_logo.png" alt="prl" width="160" ></td>
          <td width="75%" valign="top">
            <p>
              Facebook AI Research <br> 
              AI Resident<br>
              Menlo Park, CA <br>
              August, 2019 -  <br>
            </p>
            <p></p>
            <p>
              AI research team. Natural Language Processing and Speech Team. <br>
              Working with Michael Auli and Alexei Baevski on unsupervised speech pretraining algorithms.<br>
            </p>
          </td>
        </tr>

        <tr>
          <td width="25%"><img src="images/industry/nv_logo2.png" alt="prl" width="160" ></td>
          <td width="75%" valign="top">
            <p>
              Nvidia Corporation <br> 
              Research Intern<br>
              Toronto, Canada <br>
              January, 2019 - July, 2019 <br>
            </p>
            <p></p>
            <p>
              AI research team. <br>
              Under the supervison of Prof. Sanja Fidler and Prof. Antonio Torralba, working on Computer Vision projects. <br>
              Research project on Generative Adversarial Networks (GAN) and Game Simulation.
            </p>
          </td>
        </tr>

        <tr>
          <td width="25%"><img src="images/industry/intel_logo.png" alt="prl" width="160" ></td>
          <td width="75%" valign="top">
            <p>
              Intel PSG <br> 
              PEY (Professional Experience Year) Intern<br>
              San Jose, CA <br>
              May, 2017 - December, 2017 <br>
            </p>
            <p></p>
            <p>
              Participated in software development in Quartus high-level synthesis group. <br>
              Engaged in large-scale C++ programming projects on software backward compatibility. <br>
              Enhanced customers' usability to use pre-compiled products to compile on latest Quartus software. (Perl) <br>
            </p>
          </td>
        </tr>

        <tr>
          <td width="25%"><img src="images/industry/oracle_logo.png" alt="prl" width="160" ></td>
          <td width="75%" valign="top">
            <p>
              Oracle Corp. <br> 
              R&#38;D Intern<br>
              Beijing, China <br>
              June, 2015 - Aug, 2015 <br>
            </p>
            <p></p>
            <p>
              Worked in R&#38;D department cloud computing group. <br>
              Exposure to cloud-computing architecture and networking. <br>
              Utilized integrated tools to manage cloud-computing resources and services for the entire R&#38;D department. <br>
            </p>
          </td>
        </tr>

        

      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td>
          <heading>Projects</heading>
          <p>
            Some side/course projects I participated. <br>
          </p>
          </td>
        </tr>
        
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

        <tr bgcolor="#ffffff">
          <td width="25%">
            <img src="images/course_project/capstone.png" width='250'></div>   
          </td>
          <td valign="top" width="75%">
            <p>
              <papertitle>Towards a Practical sEMG Gesture Recognition System</papertitle>
              <br>
              Sebastian Kmiec*, <strong>Yuhao Zhou</strong>*<br>
              Supervisor: Stark Draper <br>
              <em>University of Toronto 4th-year Capstone Project</em>, 2019 <br>
              <b>John Senders Award</b> (1 Team across all engineering disciplines' designs) <br>
              Selected as Distinction (Top 5% among the entire student groups). <br>
              <a id='capstone_abstract', onclick="detail_switch(this.id)">Abstract</a> /
              <a href='https://www.ece.utoronto.ca/news/fourth-year-design-projects-on-display-at-ece-capstone-fair-and-showcase/'>UofT News</a> /
              <a id="midterm_link", onclick='detail_switch(this.id)'>Mid-term Presentation</a> /
              <!-- <a href='http://www.cs.toronto.edu/~henryzhou/movie4d/sources/cvpr2018.pdf'>PDF</a> / -->
              <!-- <a href="https://www.youtube.com/watch?v=O92bGCTxul8&feature=youtu.be&t=5550">CVPR Spotlight</a> / -->
              <a href="docs/capstone/final_capstone_poster.pdf">Poster</a> /<br>
              <a id="capstone_final_report", onclick='detail_switch(this.id)'>Final Report Documentation</a> /

              <br>
            </p>

            <p>
              <div id='capstone_abstract_content' style='display:none;'>
                In this project, we designed a real-time gesture recognition system for the purposes of transradial prostheses control.
                Our system makes a step towards accessible prosthesis, with hardware that is both easy to install and inexpensive. 
                The focus of our project was to collect data from two Myo armband devices, and provide highly accurate and timely gesture prediction, from a large set of predefined gestures.<br><br>
              </div>
              <div id='midterm_link_content' style='display:none;'>
                The midterm presentation updates the supervisor and project manager with progress.
                The link to the slides are available <a href="https://docs.google.com/presentation/d/19dTnNoudZA9NDioHNLlFA2y752J3l1hwfzq8ENk0XuQ/edit?usp=sharing">here</a>.<br><br>
              </div>
              <div id='capstone_final_report_content' style='display:none;'>
                To avoid potential plagiarism, the final report of the project is available upon requests.
                Thank you for the understanding.<br>
              </div>
            </p>
            <p></p>

          </td>
        </tr>

        
      </table>
      
      
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
          <td>
          <br>
          <p align="right">
            <font size="2">
            Template: <a href="https://github.com/jonbarron/jonbarron_website"><strong>This Guy</strong></a>
            </font>
          </p>
          </td>
      </tr>

      <tr>
        <td>
          <p align='center'>
            <img src="https://s04.flagcounter.com/mini/xnAl/bg_FFFFFF/txt_000000/border_FFFFFF/flags_0/" alt="Flag Counter" border="0">
          </p>
        </td>
      </tr>

      
      </table>


      <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

      </script> <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-7580334-1");
          pageTracker._trackPageview();
          } catch(err) {}
      </script>
    </td>
    </tr>
  </table>

<script>
function detail_switch(click_id) {
    click_intro = click_id + '_content'
    var x = document.getElementById(click_intro);
    if (x.style.display === "none") {
        x.style.display = "block";
    } else {
        x.style.display = "none";
    }
}
</script>

  </body>
</html>
