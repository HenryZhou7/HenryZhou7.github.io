<!--  -->
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
    <meta name=viewport content=“width=800”>
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        /* Color scheme stolen from Sergey Karayev */
        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body,
        td,
        th,
        tr,
        p,
        a {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }

        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 22px;
        }

        papertitle {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
            font-weight: 700
        }

        name {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        }

        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }

        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        span.highlight {
            background-color: #ffffd0;
        }
    </style>
    <link rel="icon" type="image/png" href="seal_icon.png">
    <title>Henry (Yuhao) Zhou</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
        type='text/css'>
</head>

<body>
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">

        <tr>
            <td>
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="67%" valign="middle">
                            <p align="center">
                                <name>Henry (Yuhao) Zhou</name>
                            </p>
                            <p>
                                I am currently pursuing entrepreneurial endeavors after finishing the Facebook (now
                                Meta)
                                <a href="https://ai.facebook.com/join-us/residency-program/">
                                    AI Residency
                                </a> program.
                                I worked with Dr. Michael Auli and Alexei Baevski on
                                unsupervised speech pretraining. <br><br>
                                During my undergraduate studies at <a href="https://www.utoronto.ca/">
                                    University of Toronto,
                                </a> my focuses are machine learning, software engineering and system control.
                                Since January in 2017, I have been working as an undergraduate research assistant under
                                the supervision
                                of
                                <a href="http://www.cs.utoronto.ca/~fidler/index.html"> Prof. Sanja Fidler </a> and
                                <a href="https://jimmylba.github.io/"> Prof. Jimmy Ba </a>
                                on computer vison and reinforcement learning projects.
                            </p>
                            <p>
                                Before I worked as a research assistant, I primarily spent my time on various software
                                engineering
                                internships.
                                During which period, I gained strong coding skills through in-depth experience on
                                large-scale
                                engineering projects.
                            </p>

                        </td>
                        <td width="33%">
                            <img src="images/profile_pic.jpg" width="200">
                        </td>



                    </tr>
                </table>
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="33%">
                            <p align=center>
                                Address: <br>
                                410-37 Grosvenor St <br>
                                Toronto, ON Canada. M4Y 3G5
                            </p>
                        </td>
                        <td width="33%">
                            <p align=center>
                                Email: <br>
                                henryzhou @ cs.toronto.edu <br>
                                henry dot zhou @mail.utoronto.ca
                            </p>
                        </td>

                    </tr>
                    <tr>
                        <p align=center>
                            <a href="docs/cv.pdf">CV</a> &nbsp/&nbsp
                            <a href="https://www.linkedin.com/in/yuhao-henry-zhou-3b0747a8/"> LinkedIn </a> &nbsp/&nbsp
                            <a href="https://github.com/HenryZhou7">GitHub</a> &nbsp/&nbsp
                            <a
                                href="https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AJsN-F4R4Qg-wL0Q0FEVJvWi1iSUfsiFNIleGJMpOu9yonslcsFONDzWjjCMNb0bhtFo6FEdzReJjEHCGZoplRA9d5f8In1uLA&user=9-6xvKYAAAAJ">Google
                                Scholar</a>
                        </p>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="100%" valign="middle">
                            <heading>Education</heading>
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr bgcolor="#ffffff">
                        <td width="25%">
                            <img src="images/education/uoft.png" width='250'></div>
                        </td>
                        <td valign="top" width="75%">
                            <p>
                                <a href="https://www.utoronto.ca/"><strong>University of Toronto</strong></a><br>
                                <a href="https://www.ece.utoronto.ca/">Faculty of Applied Science and
                                    Engineering</a><br>
                                Bachelor of Applied Science <br>
                                &nbsp &nbsp &nbsp Specialist in Electrical and Computer Engineering<br>
                                &nbsp &nbsp &nbsp <em>with</em> Robotics and Mechatronics Minor <br><br>
                                Graduated with High Honour in June, 2019.<br>
                                <em> Summa Cum Laude</em> <br>

                                <br>
                                <em>Cumulative GPA</em>: 3.92 / 4.00
                            </p>


                        </td>
                    </tr>
                </table>


                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="100%" valign="middle">
                            <heading>Research</heading>
                            <p>
                                I am broadly interested in machine learning, deep learning and its application in
                                computer vision,
                                natural language processing, speech, and locomotion control.
                                (* Denotes equal contribution.)
                            </p>
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

                    <tr bgcolor="#ffffff">
                        <td width="25%">
                            <img src="images/research/w2v-conditionalP.png" width='250'></div>
                        </td>
                        <td valign="top" width="75%">
                            <p>
                                <papertitle>A Comparison of Discrete Latent Variable Models for Speech Representation
                                    Learning
                                </papertitle>
                                <br>
                                <strong>Henry Zhou</strong>, Alexei Baevski, Michael Auli<br>
                                <em>arxiv</em>, 2020 <br>
                                <a id='w2v_vqvae_comparison_abstract' , onclick="detail_switch(this.id)">Abstract</a> /
                                <a id='w2v_vqvae_bib' onclick='detail_switch(this.id)'>Bibtex</a> /
                                <a href="https://arxiv.org/abs/2010.14230"> Arxiv</a> /
                                

                                <br>
                            </p>

                            <p>
                            <div id='w2v_vqvae_comparison_abstract_content' style='display:none;'>
                                Neural latent variable models enable the discovery of interesting structure in speech
                                audio data. This
                                paper presents a comparison of two different approaches which are broadly based on
                                predicting future
                                time-steps or auto-encoding the input signal. Our study compares the representations
                                learned by vq-vae
                                and vq-wav2vec in terms of sub-word unit discovery and phoneme recognition performance.
                                Results show
                                that future time-step prediction with vq-wav2vec achieves better performance. The best
                                system achieves
                                an error rate of 13.22 on the ZeroSpeech 2019 ABX phoneme discrimination challenge.
                            </div>

                            <div id='w2v_vqvae_bib_content' style="font-family:monospace;display:none;">
                                @misc{https://doi.org/10.48550/arxiv.2010.14230,
                                doi = {10.48550/ARXIV.2010.14230},
                                url = {https://arxiv.org/abs/2010.14230},
                                author = {Zhou, Henry and Baevski, Alexei and Auli, Michael},
                                keywords = {Audio and Speech Processing (eess.AS), Artificial Intelligence (cs.AI),
                                Machine Learning
                                (cs.LG), Sound (cs.SD), FOS: Electrical engineering, electronic engineering, information
                                engineering,
                                FOS: Electrical engineering, electronic engineering, information engineering, FOS:
                                Computer and
                                information sciences, FOS: Computer and information sciences},
                                title = {A Comparison of Discrete Latent Variable Models for Speech Representation
                                Learning},
                                publisher = {arXiv},
                                year = {2020},
                                copyright = {arXiv.org perpetual, non-exclusive license}
                                }

                                }
                            </div>
                            </p>
                            <p></p>

                        </td>
                    </tr>

                    <tr bgcolor="#ffffff">
                        <td width="25%">
                            <img src="images/research/wav2vec-2_model_pic.png" width='250'></div>
                        </td>
                        <td valign="top" width="75%">
                            <p>
                                <papertitle>wav2vec 2.0: A Framework for Self-Supervised Learning of Speech
                                    Representations</papertitle>
                                <br>
                                Alexei Baevski, <strong>Henry Zhou</strong>, Abdelrahman Mohamed, Michael Auli<br>
                                <em>arxiv</em>, 2020 <br>
                                <a id='wav2vec2_abstract' , onclick="detail_switch(this.id)">Abstract</a> /
                                <a id='wav2vec2_bib' onclick='detail_switch(this.id)'>Bibtex</a> /
                                <a href="https://arxiv.org/abs/2006.11477"> Arxiv</a> /
                                <a
                                    href="https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/">Facebook
                                    Blog</a> /
                                <a href="https://huggingface.co/docs/transformers/model_doc/wav2vec2">Hugging Face</a> / <br>
                                <a href="https://twitter.com/ylecun/status/1275915663266197504?s=20&t=YPLQHRJwivWBiaeV7k5oqA">Yann Lecun's Tweet</a>

                                <br>
                            </p>

                            <p>
                            <div id='wav2vec2_abstract_content' style='display:none;'>
                                We show for the first time that learning powerful representations from speech audio
                                alone followed by
                                fine-tuning on transcribed speech can outperform the best semi-supervised methods while
                                being
                                conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves
                                a contrastive
                                task defined over a quantization of the latent representations which are jointly
                                learned. We set a new
                                state of the art on both the 100 hour subset of Librispeech as well as on TIMIT phoneme
                                recognition.
                                When lowering the amount of labeled data to one hour, our model outperforms the previous
                                state of the
                                art on the 100 hour subset while using 100 times less labeled data. Using just ten
                                minutes of labeled
                                data and pre-training on 53k hours of unlabeled data still achieves 5.7/10.1 WER on the
                                noisy/clean test
                                sets of Librispeech. This demonstrates the feasibility of speech recognition with
                                limited amounts of
                                labeled data. Fine-tuning on all of Librispeech achieves 1.9/3.5 WER using a simple
                                baseline model
                                architecture. We will release code and models.
                            </div>

                            <div id='wav2vec2_bib_content' style="font-family:monospace;display:none;">
                                @misc{baevski2020wav2vec,
                                title={wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
                                author={Alexei Baevski and Henry Zhou and Abdelrahman Mohamed and Michael Auli},
                                year={2020},
                                eprint={2006.11477},
                                archivePrefix={arXiv},
                                primaryClass={cs.CL}
                                }
                            </div>
                            </p>
                            <p></p>

                        </td>
                    </tr>


                    <tr bgcolor="#ffffff">
                        <td width="25%">
                            <img src="images/research/gamegan-motivation.gif" width='250'></div>
                        </td>
                        <td valign="top" width="75%">
                            <p>
                                <papertitle>Learning to Simulate Dynamic Environments with GameGAN</papertitle>
                                <br>
                                Seung Wook Kim, <strong>Yuhao Zhou</strong>, Jonah Philion, Antonio Torralba, Sanja
                                Fidler<br>
                                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2020 <br>
                                <a id='gamegan_abstract' , onclick="detail_switch(this.id)">Abstract</a> /
                                <a id='gamegan_bib' onclick='detail_switch(this.id)'>Bibtex</a> /
                                <a href="https://nv-tlabs.github.io/gameGAN/">Project Web</a> /
                                <a href="https://www.nvidia.com/en-us/research/ai-playground/">Live Demos</a> <br>
                                Media:
                                <a href="https://twitter.com/nvidia/status/1263818505742200833?s=20">Nvidia Twitter</a>
                                /
                                <a href="https://blogs.nvidia.com/blog/2020/05/22/gamegan-research-pacman-anniversary/">Blog
                                    Post</a> /
                                <a
                                    href="https://www.youtube.com/watch?v=H8F6J7mYyz0&fbclid=IwAR1SxxWJBPgSzDCEWz4w11B51vSaJeE7zGELHlrSG6w_ej3l_GAGZdZLU2U">YouTube
                                    Explained </a> /
                                <a href="https://www.youtube.com/watch?v=3UZzu4UQLcI&t=343s"> Two Minute Papers </a> /
                                <a href="https://www.youtube.com/watch?v=udPY5rQVoW0">GTA Adaptation on YouTube</a> / 
                                <a href="https://venturebeat.com/ai/nvidias-gamegan-generates-games-like-pac-man-by-watching-videos/">Vecture Beat Blog</a>
                                <br>
                            </p>

                            <p>
                            <div id='gamegan_abstract_content' style='display:none;'>
                                Simulation is a crucial component of any robotic system. In order to simulate correctly,
                                we need to
                                write complex rules of the environment: how dynamic agents behave, and how the actions
                                of each of the
                                agents affect the behavior of others. In this paper, we aim to learn a simulator by
                                simply watching an
                                agent interact with an environment. We focus on graphics games as a proxy of the real
                                environment. We
                                introduce GameGAN, a generative model that learns to visually imitate a desired game by
                                ingesting
                                screenplay and keyboard actions during training. Given a key pressed by the agent,
                                GameGAN "renders" the
                                next screen using a carefully designed generative adversarial network. Our approach
                                offers key
                                advantages over existing work: we design a memory module that builds an internal map of
                                the environment,
                                allowing for the agent to return to previously visited locations with high visual
                                consistency. In
                                addition, GameGAN is able to disentangle static and dynamic components within an image
                                making the
                                behavior of the model more interpretable, and relevant for downstream tasks that require
                                explicit
                                reasoning over dynamic elements. This enables many interesting applications such as
                                swapping different
                                components of the game to build new games that do not exist. We implement our approach
                                as a web
                                application enabling human players to now play Pacman and its generated variations with
                                our GameGAN.
                            </div>

                            <div id='gamegan_bib_content' style="font-family:monospace;display:none;">
                                @inproceedings{Kim2020_GameGan,</br>
                                author = {Seung Wook Kim and Yuhao Zhou and Jonah Philion and Antonio Torralba and Sanja
                                Fidler},</br>
                                title = {{Learning to Simulate Dynamic Environments with GameGAN}},</br>
                                year = {2020},</br>
                                booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},</br>
                                month = {Jun.},</br> doi = {}</br>}
                            </div>
                            </p>
                            <p></p>

                        </td>
                    </tr>

                    <tr bgcolor="#ffffff">
                        <td width="25%">
                            <img src="http://www.cs.toronto.edu/~tingwuwang/nge_title.png" width='250'></div>
                        </td>
                        <td valign="top" width="75%">
                            <p>
                                <papertitle>Neural Graph Evolution: Automatic Robot Design</papertitle>
                                <br>
                                <strong>Yuhao Zhou</strong>*, Tingwu Wang*, Sanja Fidler, Jimmy Ba<br>
                                <em>International Conference on Learning Representations</em>, 2019 <br>
                                <a id='nge_abstract' , onclick="detail_switch(this.id)">Abstract</a> /
                                <a id='nge_bib' onclick='detail_switch(this.id)'>Bibtex</a> /
                                <a href="https://openreview.net/forum?id=BkgWHnR5tm"> Open Review</a> /
                                <a href='https://github.com/WilsonWangTHU/neural_graph_evolution'> Codes</a> /
                                <a href='http://www.cs.toronto.edu/~henryzhou/NGE_website/'>Project Web</a> /
                                <a href="https://www.youtube.com/watch?v=IRpAO839r6w">YouTube demo</a>

                                <br>
                            </p>

                            <p>
                            <div id='nge_abstract_content' style='display:none;'>
                                Despite the recent successes in robotic locomotion control, the design of robot relies
                                heavily on human
                                engineering.
                                Automatic robot design has been a long studied subject, but the recent progress has been
                                slowed due to
                                the large combinatorial search space and the difficulty in evaluating the found
                                candidates.
                                To address the two challenges, we formulate automatic robot design as a graph search
                                problem and perform
                                evolution search in graph space.
                                We propose Neural Graph Evolution (NGE), which performs selection on current candidates
                                and evolves new
                                ones iteratively.
                                Different from previous approaches, NGE uses graph neural networks to parameterize the
                                control policies,
                                which reduces evaluation cost on new candidates with the help of skill transfer from
                                previously
                                evaluated designs.
                                In addition, NGE applies Graph Mutation with Uncertainty (GM-UC) by incorporating model
                                uncertainty,
                                which reduces the search space by balancing exploration and exploitation.
                                We show that NGE significantly outperforms previous methods by an order of magnitude.
                                As shown in experiments, NGE is the first algorithm that can automatically discover
                                kinematically
                                preferred robotic graph structures, such as a fish with two symmetrical flat side-fins
                                and a tail, or a
                                cheetah with athletic front and back legs.
                                Instead of using thousands of cores for weeks, NGE efficiently solves searching problem
                                within a day on
                                a single 64 CPU-core Amazon EC2.
                            </div>

                            <div id='nge_bib_content' style="font-family:monospace;display:none;">
                                @inproceedings{
                                wang2018neural,
                                title={Neural Graph Evolution: Automatic Robot Design},
                                author={Tingwu Wang and Yuhao Zhou and Sanja Fidler and Jimmy Ba},
                                booktitle={International Conference on Learning Representations},
                                year={2019},
                                url={https://openreview.net/forum?id=BkgWHnR5tm},
                                }
                            </div>
                            </p>
                            <p></p>

                        </td>
                    </tr>

                    <tr bgcolor="#ffffff">
                        <td width="25%">
                            <img src="images/research/movie4d.jpg" width='250'></div>
                        </td>
                        <td valign="top" width="75%">
                            <p>
                                <a href="http://www.cs.toronto.edu/~henryzhou/movie4d/">
                                    <papertitle>Now You Shake Me: Towards Automatic 4D Cinema</papertitle>
                                </a><br>
                                <strong>Yuhao Zhou</strong>, Makarand Tapaswi, Sanja Fidler<br>
                                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2018
                                <strong>(Spotlight)</strong> <br>
                                <a id='movie4d_abstract' , onclick="detail_switch(this.id)">Abstract</a> /
                                <a id='movie4d_bib' onclick='detail_switch(this.id)'>Bibtex</a> /
                                <a href='http://www.cs.toronto.edu/~henryzhou/movie4d/'>Project Web</a> /
                                <a href='http://www.cs.toronto.edu/~henryzhou/movie4d/sources/cvpr2018.pdf'>PDF</a> /
                                <a href="https://www.youtube.com/watch?v=O92bGCTxul8&feature=youtu.be&t=5550">CVPR
                                    Spotlight</a> /
                                <a
                                    href="http://www.cs.toronto.edu/~henryzhou/movie4d/sources/movie4d_poster.pdf">Poster</a>
                                <br>
                                Media:
                                <a
                                    href="https://www.utoronto.ca/news/experience-what-characters-are-feeling-u-t-researchers-use-ai-add-4d-effects-movies">
                                    UofT News</a> /
                                <a href="http://www.cbc.ca/listen/shows/here-and-now-toronto/segment/15555905">CBC Radio
                                    News</a> /
                                <a
                                    href="https://www.inquisitr.com/4976832/ai-algorithm-turns-regular-movies-into-4d-motion-pictures/">Inquisitr
                                    News</a>
                            </p>

                            <p>
                            <div id='movie4d_abstract_content' style='display:none;'>
                                We are interested in enabling automatic 4D cinema by parsing physical and special
                                effects from untrimmed
                                movies.
                                These include effects such as physical interactions, water splashing, light, and
                                shaking, and are
                                grounded to either a character in the scene or the camera.
                                We collect a new dataset referred to as the Movie4D dataset which annotates over 9K
                                effects in 63
                                movies.
                                We propose a Conditional Random Field model atop a neural network that brings together
                                visual and audio
                                information, as well as semantics in the form of person tracks. Our model further
                                exploits correlations
                                of effects between different characters in the clip as well as across movie threads.
                                We propose effect detection and classification as two tasks, and present results along
                                with ablation
                                studies on our dataset, paving the way towards 4D cinema in everyone&#39;s homes.
                            </div>

                            <div id='movie4d_bib_content' style="font-family:monospace;display:none;">
                                @inproceedings{Zhou2017_Movie4D,</br>
                                author = {Yuhao Zhou and Makarand Tapaswi and Sanja Fidler},</br>
                                title = {{Now You Shake Me: Towards Automatic 4D Cinema}},</br>
                                year = {2018},</br>
                                booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},</br>
                                month = {Jun.},</br> doi = {}</br>}
                            </div>
                            </p>



                            <p></p>

                        </td>
                    </tr>
                </table>


                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td>
                            <heading>Industry Experience</heading>
                            <p>
                                Please email me for more details of the experience. <br>
                            </p>
                        </td>
                    </tr>

                </table>

                <table width="100%" align="center" border="0" cellpadding="20">

                    <tr>
                        <td width="25%"><img src="images/industry/talka-logo.png" alt="prl" width="160"></td>
                        <td width="75%" valign="top">
                            <p>
                                Talka AI <br>
                                Co-founder & Lead Architect<br>
                                Toronto, ON <br>
                                May, 2021 - Now<br>
                            </p>
                            <p></p>
                            <p>
                                A lot of different roles as in start-ups : ) <br>
                                Leading the ML and engineering team on developing novel solutions for business needs.
                                <br>
                                Participated in fund-raising, hiring, and management. <br>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td width="25%"><img src="images/industry/facebookai_logo.png" alt="prl" width="160"></td>
                        <td width="75%" valign="top">
                            <p>
                                Facebook AI Research <br>
                                AI Resident<br>
                                Menlo Park, CA <br>
                                August, 2019 - August, 2020<br>
                            </p>
                            <p></p>
                            <p>
                                AI research team. Natural Language Processing and Speech Team. <br>
                                Worked with Michael Auli and Alexei Baevski on unsupervised speech pretraining
                                algorithms.<br>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td width="25%"><img src="images/industry/nv_logo2.png" alt="prl" width="160"></td>
                        <td width="75%" valign="top">
                            <p>
                                Nvidia Corporation <br>
                                Research Intern<br>
                                Toronto, Canada <br>
                                January, 2019 - July, 2019 <br>
                            </p>
                            <p></p>
                            <p>
                                AI research team. <br>
                                Under the supervison of Prof. Sanja Fidler and Prof. Antonio Torralba, working on
                                Computer Vision
                                projects. <br>
                                Research project on Generative Adversarial Networks (GAN) and Game Simulation.
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td width="25%"><img src="images/industry/intel_logo.png" alt="prl" width="160"></td>
                        <td width="75%" valign="top">
                            <p>
                                Intel PSG <br>
                                PEY (Professional Experience Year) Intern<br>
                                San Jose, CA <br>
                                May, 2017 - December, 2017 <br>
                            </p>
                            <p></p>
                            <p>
                                Participated in software development in Quartus high-level synthesis group. <br>
                                Engaged in large-scale C++ programming projects on software backward compatibility. <br>
                                Enhanced customers' usability to use pre-compiled products to compile on latest Quartus
                                software. (Perl)
                                <br>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td width="25%"><img src="images/industry/oracle_logo.png" alt="prl" width="160"></td>
                        <td width="75%" valign="top">
                            <p>
                                Oracle Corp. <br>
                                R&#38;D Intern<br>
                                Beijing, China <br>
                                June, 2015 - Aug, 2015 <br>
                            </p>
                            <p></p>
                            <p>
                                Worked in R&#38;D department cloud computing group. <br>
                                Exposure to cloud-computing architecture and networking. <br>
                                Utilized integrated tools to manage cloud-computing resources and services for the
                                entire R&#38;D
                                department. <br>
                            </p>
                        </td>
                    </tr>



                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td>
                            <heading>Projects</heading>
                            <p>
                                Some side/course projects I participated. <br>
                            </p>
                        </td>
                    </tr>

                </table>
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

                    <tr bgcolor="#ffffff">
                        <td width="25%">
                            <img src="images/course_project/capstone.png" width='250'></div>
                        </td>
                        <td valign="top" width="75%">
                            <p>
                                <papertitle>Towards a Practical sEMG Gesture Recognition System</papertitle>
                                <br>
                                Sebastian Kmiec*, <strong>Yuhao Zhou</strong>*<br>
                                Supervisor: Stark Draper <br>
                                <em>University of Toronto 4th-year Capstone Project</em>, 2019 <br>
                                <b>John Senders Award</b> (1 Team across all engineering disciplines' designs) <br>
                                Selected as Distinction (Top 5% among the entire student groups). <br>
                                <a id='capstone_abstract' , onclick="detail_switch(this.id)">Abstract</a> /
                                <a
                                    href='https://www.ece.utoronto.ca/news/fourth-year-design-projects-on-display-at-ece-capstone-fair-and-showcase/'>UofT
                                    News</a> /
                                <a id="midterm_link" , onclick='detail_switch(this.id)'>Mid-term Presentation</a> /
                                <!-- <a href='http://www.cs.toronto.edu/~henryzhou/movie4d/sources/cvpr2018.pdf'>PDF</a> / -->
                                <!-- <a href="https://www.youtube.com/watch?v=O92bGCTxul8&feature=youtu.be&t=5550">CVPR Spotlight</a> / -->
                                <a href="docs/capstone/final_capstone_poster.pdf">Poster</a> /<br>
                                <a id="capstone_final_report" , onclick='detail_switch(this.id)'>Final Report
                                    Documentation</a> /

                                <br>
                            </p>

                            <p>
                            <div id='capstone_abstract_content' style='display:none;'>
                                In this project, we designed a real-time gesture recognition system for the purposes of
                                transradial
                                prostheses control.
                                Our system makes a step towards accessible prosthesis, with hardware that is both easy
                                to install and
                                inexpensive.
                                The focus of our project was to collect data from two Myo armband devices, and provide
                                highly accurate
                                and timely gesture prediction, from a large set of predefined gestures.<br><br>
                            </div>
                            <div id='midterm_link_content' style='display:none;'>
                                The midterm presentation updates the supervisor and project manager with progress.
                                The link to the slides are available <a
                                    href="https://docs.google.com/presentation/d/19dTnNoudZA9NDioHNLlFA2y752J3l1hwfzq8ENk0XuQ/edit?usp=sharing">here</a>.<br><br>
                            </div>
                            <div id='capstone_final_report_content' style='display:none;'>
                                To avoid potential plagiarism, the final report of the project is available upon
                                requests.
                                Thank you for the understanding.<br>
                            </div>
                            </p>
                            <p></p>

                        </td>
                    </tr>


                </table>


                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td>
                            <br>
                            <p align="right">
                                <font size="2">
                                    Template: <a href="https://github.com/jonbarron/jonbarron_website"><strong>This
                                            Guy</strong></a>
                                </font>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td>
                            <p align='center'>
                                <img src="https://s04.flagcounter.com/mini/xnAl/bg_FFFFFF/txt_000000/border_FFFFFF/flags_0/"
                                    alt="Flag Counter" border="0">
                            </p>
                        </td>
                    </tr>


                </table>


                <script type="text/javascript">
                    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
                    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

                </script>
                <script type="text/javascript">
                    try {
                        var pageTracker = _gat._getTracker("UA-7580334-1");
                        pageTracker._trackPageview();
                    } catch (err) { }
                </script>
            </td>
        </tr>
    </table>

    <script>
        function detail_switch(click_id) {
            click_intro = click_id + '_content'
            var x = document.getElementById(click_intro);
            if (x.style.display === "none") {
                x.style.display = "block";
            } else {
                x.style.display = "none";
            }
        }
    </script>

</body>

</html>